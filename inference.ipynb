{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaojw/anaconda3/envs/torch1.12/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/zhaojw/anaconda3/envs/torch1.12/lib/python3.9/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AccoMontage piano texture search space. This may take 1 or 2 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:02<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading orchestration prompt search space ...\n",
      "loading Slakh2100 Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:10<00:00, 20.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize model ...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import numpy as np\n",
    "import pretty_midi as pyd\n",
    "from arrangement_utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\"\"\"Download checkpoints and data at https://drive.google.com/drive/folders/17yB-Oae_4eGKJmqRS-LB8PwE2rqwZrUu?usp=sharing (582MB) and decompress at the directory\"\"\"\n",
    "DATA_FILE_ROOT = './data_file_dir/'\n",
    "DEVICE = 'cuda:0'\n",
    "piano_arranger, orchestrator, piano_texture, band_prompt = load_premise(DATA_FILE_ROOT, DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize input and preference\n",
    "We provide four sample lead sheets for a quick inference. You should be able to directly run the code blocks after downloading the pre-trained checkpoints.\n",
    "\n",
    "If you wish to test our model on your own lead sheet file, please initialize a sub-folder with its `SONG_NAME` in the `./demo` folder and put the file in, and name the file \"lead sheet.mid\". \n",
    "\n",
    "Please also specify `SEGMENTATION` (phrase structure) and `NOTE_SHIFT` (the duration of the pick-up measure if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set input lead sheet\"\"\"\n",
    "#SONG_NAME, SEGMENTATION, NOTE_SHIFT, TEMPO = 'Castles in the Air', 'A8A8B8B8', 1 , 100  #1 beat in the pick-up measure\n",
    "#SONG_NAME, SEGMENTATION, NOTE_SHIFT, TEMPO = 'Jingle Bells', 'A8B8A8', 0, 100\n",
    "#SONG_NAME, SEGMENTATION, NOTE_SHIFT, TEMPO = 'Sally Garden', 'A4A4B4A4', 0, 75\n",
    "SONG_NAME, SEGMENTATION, NOTE_SHIFT, TEMPO = 'Auld Lang Syne', 'A8B8A8B8', 1, 80\n",
    "\n",
    "\"\"\"Set texture pre-filtering for piano arrangement (default random)\"\"\"\n",
    "RHTHM_DENSITY = np.random.randint(2, 5)\n",
    "VOICE_NUMBER = np.random.randint(2, 5)\n",
    "PREFILTER = (RHTHM_DENSITY, VOICE_NUMBER)\n",
    "\n",
    "\"\"\"Set if use a 2-bar prompt for full-band arrangement (default True)\"\"\" \n",
    "USE_PROMPT = True\n",
    "\n",
    "lead_sheet = read_lead_sheet('./demo', SONG_NAME, SEGMENTATION, NOTE_SHIFT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piano Accompaniment Arrangement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrasal Unit selection begins:\n",
      "\t 4 phrases in the lead sheet;\n",
      "\t set note density filter: (3, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:10<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-harmonization begins ...\n",
      "Piano accompaiment generated!\n"
     ]
    }
   ],
   "source": [
    "midi_piano, acc_piano = piano_arrangement(*lead_sheet, *piano_texture, piano_arranger, PREFILTER, TEMPO)\n",
    "midi_piano.write(f'./demo/{SONG_NAME}/arrangement_piano.mid')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior model initialized with 6 tracks:\n",
      "\t['Soprano/Alto Sax', 'Choir and Voice', 'Clean Electric Guitar', 'Acoustic Guitar', 'Acoustic Piano', 'Electric Bass']\n"
     ]
    }
   ],
   "source": [
    "func_prompt = prompt_sampling(acc_piano, *band_prompt, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orchestration begins ...\n",
      "Full-band accompaiment generated!\n"
     ]
    }
   ],
   "source": [
    "if USE_PROMPT:\n",
    "    midi_band = orchestration(acc_piano, None, *func_prompt, orchestrator, DEVICE, blur=.25, p=.05, t=6, tempo=TEMPO)\n",
    "else:\n",
    "    instruments, time_promt = func_prompt\n",
    "    midi_band = orchestration(acc_piano, None, instruments, None, orchestrator, DEVICE, blur=.25, p=.05, t=8, tempo=TEMPO)\n",
    "mel_track = pyd.Instrument(program=72, is_drum=False, name='melody')\n",
    "mel_track.notes = midi_piano.instruments[0].notes\n",
    "midi_band.instruments.append(mel_track)\n",
    "midi_band.write(f'./demo/{SONG_NAME}/arrangement_band.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.10_conda11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
